#  深度学习可解释性
说起来点燃我对深度学习研究兴趣的就是这个方向的一篇文章，作者是上交的张拳石老师。文章里张老师极尽描绘可解释性的重要性与必然性，表示可解释性才是足以支撑起人工智能的未来的奠基石。
读到此会让人觉得心潮澎湃，恨不得立马投身这伟大的事业，深耕数十年然后做出足以颠覆整个学术界的成果，然后名列历史课本。

但是事实上可解释性可能跟大家最开始想的都不一样。

首先的第一点，深度学习可解释性和深度学习理论是两回事。前者更多的工作是去对现有的黑箱模型，用各种方法去揭示模型内参数的变化，而这个过程往往要损害模型的效果。
所以有学术大佬评价说可解释性只是在模型效果不够好的时候才需要，用可解释方法去引导模型改进。而当前深度学习领域基于transformers的诸多主流模型还没触及到能力的上限，更何况学术界需要的与其说是对transformers进行可解释性分析以提高效果，
不如说更想需要一个崭新的可以作为继CNN，RNN，transformers之后的新的基石模型。

而后者深度学习理论，这个领域其实也不像大家想象的那样，首先出人意料的是从深度学习火热起来到现在，深度学习理论其实很少人做。然后这个领域大家基本上都是各做各的，很少有一个方向可以有很多人不断的投入不断的去积累。就我了解的而言，大概有Neural Kernel（NTK），
Feature learning theory，还有 香港大学马毅老师的用压缩理解深度学习（这位是认为自己的研究是彻底揭示了深度学习的本质，大家可以去了解了解）。大家都在用各自的数学方法在深度学习理论各显身手，用各种数学方法想要去揭示深度学习模型的本质，
但是确实而言没有什么非常有意义的进展。

然后第二点，可解释性也分实用和理论，实用就是拿大佬们研究出来的可解释性算法用到不同的模型上，去研究哪种方法会效果比较好。而理论就是自己去研究为什么这样的算法，什么样的算法效果会好。

不过可解释性方向确实做的人逐渐变多了，相关的信息也好找，但是如果以后不走学术道路建议不要走这个方向。一是学校里应该是没有老师做，二是工业界很少对口岗位，三是真的很难做出什么在工业界有意义的成果出来。

