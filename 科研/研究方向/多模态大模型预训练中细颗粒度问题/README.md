# 多模态模型预训练中细颗粒度对齐问题
细颗粒度的对齐问题是多模态模型预训练的核心问题，主要是需要在不同模态的表征空间里进行更细致的对齐。我的理解是要从不同模态的知识表示本质来探索，目前多是用几个固定的预训练方法，比如image-text对比loss，还有自监督的generation-mask任务。有可能最终还是要走到对基础网络架构的创新，出现一个崭新的类似transformers的模型或者是对transformers模型的基本理论突破。
